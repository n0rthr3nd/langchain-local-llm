# LangChain + Ollama en Docker

Entorno completo para desarrollar con LangChain usando LLMs locales sin costes.

## üöÄ Plataformas Soportadas

- **Windows** (Docker Desktop con WSL2)
- **Linux** (x86_64 y ARM64)
- **macOS** (Intel y Apple Silicon)
- **ü•ß Raspberry Pi 5** (8GB RAM) - [Ver gu√≠a espec√≠fica](RASPBERRY_PI_SETUP.md)
- **‚ò∏Ô∏è Kubernetes (k3s)** - [Ver gu√≠a de despliegue](K8S_QUICKSTART.md) | [Documentaci√≥n completa](K8S_DEPLOYMENT.md)

## Requisitos Previos

### Windows / macOS / Linux (x86_64)
- **Docker Desktop** o Docker Engine
- **16 GB RAM** recomendado (8 GB m√≠nimo)
- **10 GB espacio en disco** para modelos

### Raspberry Pi 5
- **8GB RAM** (recomendado)
- **Docker** instalado
- **32GB+ microSD** o SSD USB
- Ver [RASPBERRY_PI_SETUP.md](RASPBERRY_PI_SETUP.md) para gu√≠a completa

## Inicio R√°pido

### ü•ß Para Raspberry Pi 5

**Usa la configuraci√≥n optimizada para ARM64:**

```bash
# Instalaci√≥n autom√°tica (recomendado)
chmod +x scripts/setup_rpi.sh
./scripts/setup_rpi.sh

# O manualmente:
docker compose -f docker-compose.rpi.yml up -d
docker exec ollama-server ollama pull gemma2:2b
```

üìñ **Gu√≠a completa:** [RASPBERRY_PI_SETUP.md](RASPBERRY_PI_SETUP.md)

---

### ‚ò∏Ô∏è Para Kubernetes (k3s)

**Despliegue en cluster k3s con auto-scaling y alta disponibilidad:**

```bash
# 1. Construir e importar imagen
./k8s/scripts/build-and-push.sh

# 2. Desplegar servicios
./k8s/scripts/deploy.sh

# 3. Descargar modelos
kubectl apply -f k8s/base/model-download-job.yaml
kubectl logs -n llm-services job/model-download -f

# 4. Verificar
kubectl get pods -n llm-services
```

**Acceso Web:** `https://northr3nd.duckdns.org/ia/chat`

üìñ **Gu√≠as:**
- [Quick Start (5 minutos)](K8S_QUICKSTART.md)
- [Documentaci√≥n completa](K8S_DEPLOYMENT.md)
- [Ejemplos de integraci√≥n](k8s/EXAMPLES.md)

**Ventajas del despliegue en k3s:**
- ‚úÖ Alta disponibilidad (m√∫ltiples r√©plicas de la API)
- ‚úÖ Auto-scaling basado en CPU/memoria
- ‚úÖ Balanceo de carga autom√°tico
- ‚úÖ Integraci√≥n nativa con otros servicios del cluster
- ‚úÖ Actualizaciones rolling sin downtime
- ‚úÖ NetworkPolicies para seguridad

---

### üíª Para Windows / macOS / Linux

### 1. Clonar y levantar servicios

```bash
# Iniciar solo Ollama primero
docker compose up -d ollama

# Levantar todos los servicios
docker-compose up -d
```

### 2. Descargar modelos

```bash
# Modelo principal (4.7 GB)
docker exec ollama-server ollama pull llama3.2

# Modelos alternativos (opcional)
docker exec ollama-server ollama pull mistral
docker exec ollama-server ollama pull phi3:mini

# Verificar modelos instalados
docker exec ollama-server ollama list
```

### 3. Iniciar Aplicaci√≥n

```bash
# Iniciar todo
docker compose up -d

# Ver logs
docker compose logs -f langchain-app
```

La interfaz web estar√° lista para usar. El backend API est√° en `http://localhost:8000`.

### 4. Ver logs

```bash
# Ejemplos b√°sicos
docker exec -it langchain-app python main.py

# Ejemplo RAG
docker exec -it langchain-app python rag_example.py

# Iniciar API REST
docker exec -it langchain-app python api_server.py
```

## Endpoints de la API

### GET /models

Lista modelos disponibles en Ollama.

```bash
curl http://localhost:8000/models
```

### POST /chat

Chat sin streaming (respuesta completa).

```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hola, ¬øc√≥mo est√°s?"}
    ],
    "model": "llama3.2",
    "temperature": 0.7,
    "max_tokens": 512
  }'
```

### POST /chat/stream

Chat con streaming (tokens en tiempo real).

```bash
curl -X POST http://localhost:8000/chat/stream \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Escribe un poema corto"}
    ],
    "model": "llama3.2",
    "temperature": 0.9
  }'
```

## Modelos Disponibles

### Para PC / Laptop (16GB+ RAM)

| Modelo | Tama√±o | RAM Necesaria | Uso Recomendado |
|--------|--------|---------------|-----------------|
| `llama3.2` | 4.7 GB | 16 GB | Uso general |
| `mistral` | 4.1 GB | 16 GB | Buen balance |
| `llama3.1:70b` | 40 GB | 64 GB | Alta calidad |

### Para Raspberry Pi / 8GB RAM

| Modelo | Tama√±o | RAM Necesaria | Uso Recomendado |
|--------|--------|---------------|-----------------|
| `gemma2:2b` | 2.7 GB | 6 GB | ‚úÖ Recomendado para RPI |
| `phi3:mini` | 2.3 GB | 6 GB | C√≥digo y razonamiento |
| `llama3.2:3b` | 2.0 GB | 5 GB | Tareas simples |
| `tinyllama` | 600 MB | 3 GB | Ultra ligero |

Para cambiar de modelo:

```bash
# Descargar nuevo modelo
docker exec ollama-server ollama pull mistral

# Configurar en .env
# MODEL_NAME=mistral
```

## Estructura del Proyecto

```
langchain-local-llm/
‚îú‚îÄ‚îÄ docker-compose.yml        # Configuraci√≥n para PC/Laptop
‚îú‚îÄ‚îÄ docker-compose.rpi.yml    # ü•ß Configuraci√≥n para Raspberry Pi
‚îú‚îÄ‚îÄ Dockerfile                # Imagen multi-arquitectura
‚îú‚îÄ‚îÄ requirements.txt          # Dependencias Python
‚îú‚îÄ‚îÄ .env.example             # Variables de entorno (PC)
‚îú‚îÄ‚îÄ .env.rpi                 # ü•ß Variables de entorno (RPI)
‚îú‚îÄ‚îÄ RASPBERRY_PI_SETUP.md    # ü•ß Gu√≠a completa para RPI
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # Ejemplos b√°sicos
‚îÇ   ‚îú‚îÄ‚îÄ rag_example.py       # Ejemplo RAG completo
‚îÇ   ‚îú‚îÄ‚îÄ agent_example.py     # Agentes con herramientas
‚îÇ   ‚îî‚îÄ‚îÄ api_server.py        # API REST con FastAPI
‚îî‚îÄ‚îÄ scripts/
    ‚îú‚îÄ‚îÄ setup.ps1            # Script Windows
    ‚îî‚îÄ‚îÄ setup_rpi.sh         # ü•ß Script para Raspberry Pi
```

## Uso con GPU NVIDIA

Si tienes GPU NVIDIA, el rendimiento ser√° mucho mejor.

### Requisitos

1. Drivers NVIDIA actualizados
2. [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)

### Verificar

```powershell
# Verificar que Docker puede ver la GPU
docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi
```

El docker-compose.yml ya est√° configurado para usar GPU. Si no tienes GPU, comenta estas l√≠neas:

```yaml
# deploy:
#   resources:
#     reservations:
#       devices:
#         - driver: nvidia
#           count: all
#           capabilities: [gpu]
```

## Funcionalidades de la UI

### Gesti√≥n de Conversaciones

- **Nueva conversaci√≥n**: Bot√≥n en la barra lateral
- **Cambiar conversaci√≥n**: Click en conversaci√≥n guardada
- **Eliminar conversaci√≥n**: Bot√≥n de basura (hover)
- **Auto-guardado**: Las conversaciones se guardan autom√°ticamente en localStorage

### Controles de Chat

- **Enviar mensaje**: Enter (Shift+Enter para nueva l√≠nea)
- **Stop generaci√≥n**: Bot√≥n de stop durante streaming
- **Regenerar**: Regenera la √∫ltima respuesta
- **Limpiar chat**: Elimina todos los mensajes

### Configuraci√≥n

- **Modelo**: Selecciona entre modelos disponibles
- **Temperature**: 0.0 (determinista) a 2.0 (creativo)
- **Max Tokens**: Longitud m√°xima de respuesta
- **System Prompt**: Instrucciones para el asistente

### Export/Import

Las conversaciones se guardan en localStorage y pueden exportarse/importarse manualmente desde el navegador.

## Troubleshooting

### Ollama no responde

```powershell
# Reiniciar contenedor
docker-compose restart ollama

# Ver logs
docker logs ollama-server

# Verificar que est√° corriendo
docker exec ollama-server ollama list
```

### Error de memoria

```powershell
# Aumentar memoria de Docker Desktop
# Settings > Resources > Memory > 8 GB o m√°s

# O usar un modelo m√°s peque√±o
docker exec ollama-server ollama pull phi3:mini
```

### Frontend no carga

```powershell
# Verificar que el contenedor est√° corriendo
docker ps | grep chatgpt-web

# Reconstruir frontend
docker-compose up -d --build web

# Ver logs
docker logs chatgpt-web
```

### Backend error 503

```powershell
# Verificar que Ollama est√° corriendo
docker ps | grep ollama-server

# Verificar conectividad desde el backend
docker exec langchain-app curl http://ollama:11434/api/tags
```

### Puerto 3000 ya en uso

Cambia el puerto en `docker-compose.yml`:

```yaml
ports:
  - "3001:80"  # Cambiar 3000 a 3001
```

## Despliegue en Producci√≥n

Para producci√≥n, considera:

1. **HTTPS**: Usa un proxy reverso (nginx, Caddy, Traefik)
2. **Autenticaci√≥n**: Implementa auth en el backend
3. **Rate limiting**: Limita peticiones por IP
4. **CORS**: Restringe or√≠genes permitidos
5. **Logs**: Configura logging centralizado
6. **Backup**: Backup de modelos y datos

## Licencia

Este proyecto es de c√≥digo abierto. Ver LICENSE para m√°s detalles.

## Recursos

- [Ollama](https://ollama.ai/)
- [LangChain](https://python.langchain.com/)
- [FastAPI](https://fastapi.tiangolo.com/)
- [React](https://react.dev/)
- [Vite](https://vitejs.dev/)

## Contribuir

Las contribuciones son bienvenidas. Por favor:

1. Fork el proyecto
2. Crea una rama para tu feature
3. Commit tus cambios
4. Push a la rama
5. Abre un Pull Request
