# Configuracion de entorno para LangChain + Ollama
# Copiar a .env y ajustar valores

# URL del servidor Ollama
OLLAMA_BASE_URL=http://ollama:11434

# Modelo principal para chat/generacion
MODEL_NAME=llama3.2

# Modelo para embeddings (RAG)
EMBEDDING_MODEL=nomic-embed-text

# Configuracion de la API
API_HOST=0.0.0.0
API_PORT=8000

# Nivel de logging
LOG_LEVEL=INFO
