# HorizontalPodAutoscaler para LangChain API
# Escala automáticamente según el uso de CPU/memoria
#
# NOTA: Requiere metrics-server instalado en k3s
# Instalar con: kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: langchain-api-hpa
  namespace: llm-services
  labels:
    app.kubernetes.io/part-of: langchain-ollama
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: langchain-api
  minReplicas: 1
  maxReplicas: 1  # Máximo 4 réplicas (limitado por RAM disponible)
  metrics:
  # Escalar basado en CPU
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70  # Escalar cuando CPU > 70%
  # Escalar basado en memoria
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80  # Escalar cuando memoria > 80%
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300  # Esperar 5 min antes de reducir
      policies:
      - type: Percent
        value: 50  # Reducir máximo 50% de réplicas a la vez
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0  # Escalar inmediatamente
      policies:
      - type: Percent
        value: 100  # Duplicar réplicas si es necesario
        periodSeconds: 15
      - type: Pods
        value: 2  # Añadir máximo 2 pods a la vez
        periodSeconds: 15
      selectPolicy: Max
